{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1.What does R-squared represent in a regression model?\n",
        "\n",
        "R-squared measures the proportion of the variance in the dependent variable that is explained by the independent variables in the model. A higher R-squared indicates that the model explains a larger portion of the variability in the data."
      ],
      "metadata": {
        "id": "gzyv8VVSoNbM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.What are the assumptions of linear regression?\n",
        "\n",
        "The key assumptions of linear regression include:\n",
        "\n",
        "Linearity: The relationship between the independent and dependent variables is linear.\n",
        "Independence: Observations are independent of one another.\n",
        "Homoscedasticity: The variance of the residuals is constant across all levels of the independent variable(s).\n",
        "Normality: The residuals should be approximately normally distributed.\n",
        "No multicollinearity: The independent variables should not be highly correlated."
      ],
      "metadata": {
        "id": "QrteWOFRoWUM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.What is the difference between R-squared and Adjusted R-squared?\n",
        "\n",
        "R-squared represents the proportion of variance explained by the model, but it can increase with the addition of more predictors, even if they don‚Äôt improve the model. Adjusted R-squared adjusts for the number of predictors and provides a more reliable measure when comparing models with different numbers of predictors."
      ],
      "metadata": {
        "id": "q3aMcLA4obeY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.Why do we use Mean Squared Error (MSE)?\n",
        "\n",
        "MSE is used to measure the average squared difference between the actual and predicted values. It penalizes larger errors more heavily, making it useful for identifying and quantifying the magnitude of errors in the predictions.\n",
        "\n"
      ],
      "metadata": {
        "id": "r9e0yQwHo4bH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.What does an Adjusted R-squared value of 0.85 indicate?\n",
        "\n",
        "An Adjusted R-squared of 0.85 means that 85% of the variance in the dependent variable is explained by the independent variables, adjusting for the number of predictors. This suggests a strong model fit, but it‚Äôs important to verify the quality of the model through other metrics and diagnostics."
      ],
      "metadata": {
        "id": "Zm65xCWCpMaw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.How do we check for normality of residuals in linear regression?\n",
        "\n",
        "To check for normality of residuals, you can use:\n",
        "\n",
        "A histogram of the residuals.\n",
        "A Q-Q (quantile-quantile) plot.\n",
        "Statistical tests like the Shapiro-Wilk test or Anderson-Darling test for normality."
      ],
      "metadata": {
        "id": "2klciKGWpQ9F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.What is multicollinearity, and how does it impact regression?\n",
        "\n",
        "Multicollinearity occurs when two or more independent variables are highly correlated, making it difficult to assess the individual effect of each predictor. It can lead to unstable estimates of coefficients, large standard errors, and misleading statistical significance."
      ],
      "metadata": {
        "id": "x2ERFkYLpVvk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.What is Mean Absolute Error (MAE)?\n",
        "MAE measures the average magnitude of the errors in a set of predictions, without considering their direction (positive or negative). It gives a more interpretable measure of error since it doesn‚Äôt square the differences, unlike MSE."
      ],
      "metadata": {
        "id": "Xpmetl22pcCb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.What are the benefits of using an ML pipeline?\n",
        "\n",
        "ML pipelines automate the process of data preparation, model training, and evaluation, ensuring consistency, reproducibility, and efficiency. They help reduce human error, facilitate model tuning, and allow for easy deployment."
      ],
      "metadata": {
        "id": "tH4zxieOphga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "H3JNzsYXrpry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.Why is RMSE considered more interpretable than MSE?\n",
        "RMSE (Root Mean Squared Error) is considered more interpretable than MSE because it is in the same units as the dependent variable. MSE, on the other hand, is in squared units, which makes it harder to interpret in the context of the data."
      ],
      "metadata": {
        "id": "dWLVevuNplDv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.What alternatives exist to pickling for saving ML models?\n",
        "Alternatives to pickling include joblib, which is more efficient for large models, and TensorFlow SavedModel for TensorFlow-based models. ONNX allows for cross-framework model sharing, and HDF5 is commonly used in Keras. These methods provide better performance, compatibility, and are suited for production environments"
      ],
      "metadata": {
        "id": "lMU-8Ue1pmq1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.What is heteroscedasticity, and why is it a problem?\n",
        "Heteroscedasticity occurs when the variance of residuals is not constant across levels of the independent variable. It undermines the reliability of regression models by biasing standard errors, leading to incorrect p-values and confidence intervals. Detecting it involves plotting residuals or using tests like Breusch-Pagan, and it can be addressed using transformations or weighted least squares."
      ],
      "metadata": {
        "id": "bep_fT6mqlyB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.How does adding irrelevant predictors affect R-squared and Adjusted R-squared?\n",
        "Adding irrelevant predictors increases R-squared, but it doesn't improve the model's explanatory power. Adjusted R-squared accounts for the number of predictors and penalizes irrelevant ones, potentially decreasing if unnecessary variables are added."
      ],
      "metadata": {
        "id": "c3x3dkjXqzL8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14.What is pickling in Python, and how is it useful in ML?\n",
        "Pickling in Python serializes objects (like trained models) into a byte stream, enabling storage and later reloading. It‚Äôs useful for saving machine learning models and sharing them across different environments or sessions."
      ],
      "metadata": {
        "id": "r4R6mj8Fq1MX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.What does a high R-squared value mean?\n",
        "A high R-squared indicates that the independent variables explain a large portion of the variance in the dependent variable, suggesting a good fit. However, it doesn't guarantee the model is ideal or free from overfitting."
      ],
      "metadata": {
        "id": "5D3-P8TNq4oI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.What happens if linear regression assumptions are violated?\n",
        "Violating linear regression assumptions (like linearity, independence, and normality of residuals) can lead to biased estimates, unreliable predictions, and invalid inferences, reducing model accuracy."
      ],
      "metadata": {
        "id": "VMHEMShLq7d2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.How can we address multicollinearity in regression?\n",
        "Multicollinearity can be addressed by removing highly correlated predictors, combining them, or using techniques like Ridge or Lasso regression, which apply regularization to reduce its impact."
      ],
      "metadata": {
        "id": "9cYYGN5pq_0Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18.Why do we use pipelines in machine learning?\n",
        "Pipelines automate and streamline processes like data preprocessing, model training, and evaluation. They ensure consistency, reproducibility, and efficiency, making model development and deployment easier."
      ],
      "metadata": {
        "id": "N0ZueZt0rB9N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19.How is Adjusted R-squared calculated?\n",
        "Adjusted R-squared is calculated by adjusting the R-squared value for the number of predictors and observations, penalizing the inclusion of irrelevant variables:\n",
        "1\n",
        "‚àí\n",
        "(\n",
        "(\n",
        "1\n",
        "‚àí\n",
        "ùëÖ\n",
        "2\n",
        ")\n",
        "(\n",
        "ùëõ\n",
        "‚àí\n",
        "1\n",
        ")\n",
        "ùëõ\n",
        "‚àí\n",
        "ùëù\n",
        "‚àí\n",
        "1\n",
        ")\n",
        "1‚àí(\n",
        "n‚àíp‚àí1\n",
        "(1‚àíR\n",
        "2\n",
        " )(n‚àí1)\n",
        "‚Äã\n",
        " )"
      ],
      "metadata": {
        "id": "diZc3ZNtrGIq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20.Why is MSE sensitive to outliers?\n",
        "MSE squares the residuals, which means large errors (outliers) are disproportionately weighted. This makes the model more sensitive to extreme values, potentially distorting performance metrics."
      ],
      "metadata": {
        "id": "KR_tn0VSrKIq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21.Why is MSE sensitive to outliers?\n",
        "MSE squares the residuals, which means large errors (outliers) are disproportionately weighted. This makes the model more sensitive to extreme values, potentially distorting performance metrics."
      ],
      "metadata": {
        "id": "PlZkDdrzrT2M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22.What is Root Mean Squared Error (RMSE)?\n",
        "RMSE is the square root of MSE, providing a measure of average error in the same units as the dependent variable. It‚Äôs often used because it‚Äôs more interpretable and gives a clearer sense of prediction accuracy."
      ],
      "metadata": {
        "id": "rRuf37NjrXtD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23.Why is pickling considered risky?\n",
        "Pickling can be risky because it‚Äôs not secure against arbitrary code execution, especially with untrusted data. It also may cause compatibility issues when loading models across different Python versions."
      ],
      "metadata": {
        "id": "0ln0Mg5uraXZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24.What alternatives exist to pickling for saving ML models?\n",
        "Alternatives include joblib (more efficient for large models), ONNX (for cross-framework compatibility), and HDF5 (used with Keras), which offer better compatibility, performance, and security than pickling."
      ],
      "metadata": {
        "id": "fP7XQokxrdtc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25.What is heteroscedasticity, and why is it a problem?\n",
        "Heteroscedasticity occurs when the residuals have non-constant variance, which violates the assumption of linear regression. It can lead to inefficient estimates and incorrect inferences, making the model unreliable."
      ],
      "metadata": {
        "id": "b3m14YJyrfvp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CODES**"
      ],
      "metadata": {
        "id": "xrBG7Vler-zf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Load dataset\n",
        "diamonds = sns.load_dataset('diamonds')\n",
        "\n",
        "# We will use 'carat' and 'depth' as features to predict 'price'\n",
        "diamonds = diamonds.dropna(subset=['price', 'carat', 'depth'])\n",
        "\n",
        "# Feature selection\n",
        "X = diamonds[['carat', 'depth']]\n",
        "y = diamonds['price']\n",
        "\n",
        "# Split data into training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and fit the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate MSE and MAE\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "print(f\"Mean Absolute Error: {mae}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLCM2qxosHEr",
        "outputId": "c828a29d-1764-4182-f66a-455a9c40dac4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 2375577.9174803584\n",
            "Mean Absolute Error: 1005.842371113657\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import numpy as np\n",
        "\n",
        "# Load the 'tips' dataset\n",
        "tips = sns.load_dataset('tips')\n",
        "\n",
        "# Feature selection\n",
        "X = tips[['total_bill', 'size']]\n",
        "y = tips['tip']\n",
        "\n",
        "# Split data into training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and fit the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate MSE, MAE, and RMSE\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "print(f\"Mean Absolute Error: {mae}\")\n",
        "print(f\"Root Mean Squared Error: {rmse}\")\n"
      ],
      "metadata": {
        "id": "Ho1fjt45sQJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the dataset\n",
        "diamonds = sns.load_dataset('diamonds')\n",
        "\n",
        "# Drop rows with missing values in 'price', 'carat', and 'depth'\n",
        "diamonds = diamonds.dropna(subset=['price', 'carat', 'depth'])\n",
        "\n",
        "# Scatter plot to check linearity between 'carat' and 'price'\n",
        "sns.scatterplot(x='carat', y='price', data=diamonds)\n",
        "plt.title('Scatter Plot - Linearity Check')\n",
        "plt.show()\n",
        "\n",
        "# Fit a linear regression model\n",
        "X = diamonds[['carat', 'depth']]\n",
        "y = diamonds['price']\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Residuals plot for homoscedasticity\n",
        "y_pred = model.predict(X)\n",
        "residuals = y - y_pred\n",
        "sns.residplot(x=y_pred, y=residuals, lowess=True, line_kws={'color': 'red'})\n",
        "plt.title('Residuals Plot - Homoscedasticity Check')\n",
        "plt.show()\n",
        "\n",
        "# Correlation matrix for multicollinearity check\n",
        "corr_matrix = X.corr()\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Matrix - Multicollinearity Check')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "988aDTHssRNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load dataset\n",
        "diamonds = sns.load_dataset('diamonds')\n",
        "\n",
        "# Drop rows with missing values\n",
        "diamonds = diamonds.dropna(subset=['price', 'carat', 'depth'])\n",
        "\n",
        "# Feature selection\n",
        "X = diamonds[['carat', 'depth']]\n",
        "y = diamonds['price']\n",
        "\n",
        "# Split data into training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('regressor', LinearRegression())\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R-squared: {r2}\")\n"
      ],
      "metadata": {
        "id": "Syip9UdtsTMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset\n",
        "tips = sns.load_dataset('tips')\n",
        "\n",
        "# Feature selection\n",
        "X = tips[['total_bill']]\n",
        "y = tips['tip']\n",
        "\n",
        "# Split data into training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and fit the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Print the model's coefficients, intercept, and R-squared score\n",
        "print(f\"Coefficient: {model.coef_}\")\n",
        "print(f\"Intercept: {model.intercept_}\")\n",
        "print(f\"R-squared: {model.score(X_test, y_test)}\")\n"
      ],
      "metadata": {
        "id": "Oj-ExvmGsWB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Load the dataset\n",
        "tips = sns.load_dataset('tips')\n",
        "\n",
        "# Feature selection\n",
        "X = tips[['total_bill']]\n",
        "y = tips['tip']\n",
        "\n",
        "# Create and fit the model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Print the slope (coefficient) and intercept of the regression line\n",
        "print(f\"Slope (Coefficient): {model.coef_}\")\n",
        "print(f\"Intercept: {model.intercept_}\")\n"
      ],
      "metadata": {
        "id": "dKT3NRsRsYlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Generate synthetic data\n",
        "X = np.random.rand(100, 1) * 10  # 100 random values between 0 and 10\n",
        "y = 2 * X + 3 + np.random.randn(100, 1)  # Linear relation with noise\n",
        "\n",
        "# Create and fit the model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "# Plot the data points and the regression line\n",
        "plt.scatter(X, y, color='blue', label='Data points')\n",
        "plt.plot(X, y_pred, color='red', label='Regression line')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Print the coefficient and intercept\n",
        "print(f\"Coefficient: {model.coef_}\")\n",
        "print(f\"Intercept: {model.intercept_}\")\n"
      ],
      "metadata": {
        "id": "dmOozYwksa-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Create and fit the model\n",
        "model = LinearRegression()\n",
        "model.fit([[1], [2], [3]], [1, 2, 3])  # Example data\n",
        "\n",
        "# Save the model using pickle\n",
        "with open('linear_regression_model.pkl', 'wb') as f:\n",
        "    pickle.dump(model, f)\n"
      ],
      "metadata": {
        "id": "RSwZfS2GsdQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Generate synthetic data\n",
        "X = np.random.rand(100, 1) * 10\n",
        "y = 2 * X**2 + 3 + np.random.randn(100, 1)\n",
        "\n",
        "# Transform features to include polynomial terms (degree 2)\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Fit polynomial regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_poly)\n",
        "\n",
        "# Plot the data points and the polynomial regression curve\n",
        "plt.scatter(X, y, color='blue')\n",
        "plt.plot(X, y_pred, color='red')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.title('Polynomial Regression (Degree 2)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QaJyjB20se8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Generate synthetic data\n",
        "X = np.random.rand(100, 1) * 10\n",
        "y = 3 * X + 5 + np.random.randn(100, 1)\n",
        "\n",
        "# Fit the model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Print the coefficient and intercept\n",
        "print(f\"Coefficient: {model.coef_}\")\n",
        "print(f\"Intercept: {model.intercept_}\")\n"
      ],
      "metadata": {
        "id": "7kSuCJGMshAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Generate synthetic data\n",
        "X = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
        "y = X**3 + 2*X**2 + 1 + np.random.randn(100, 1) * 3  # Non-linear relationship with noise\n",
        "\n",
        "# Transform features to include polynomial terms (degree 3)\n",
        "poly = PolynomialFeatures(degree=3)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Fit polynomial regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_poly)\n",
        "\n",
        "# Plot the data points and the polynomial regression\n"
      ],
      "metadata": {
        "id": "SBgIEDwdsv7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset\n",
        "tips = sns.load_dataset('tips')\n",
        "\n",
        "# Feature selection\n",
        "X = tips[['total_bill', 'size']]\n",
        "y = tips['tip']\n",
        "\n",
        "# Split data into training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and fit the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Print the model's coefficients, intercept, and R-squared score\n",
        "print(f\"Coefficients: {model.coef_}\")\n",
        "print(f\"Intercept: {model.intercept_}\")\n",
        "print(f\"R-squared: {model.score(X_test, y_test)}\")\n"
      ],
      "metadata": {
        "id": "F-qxhZATsxg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate synthetic data\n",
        "X = np.random.rand(100, 1) * 10\n",
        "y = 2 * X + 3 + np.random.randn(100, 1)\n",
        "\n",
        "# Split data into training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and fit the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate MSE, MAE, and RMSE\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "print(f\"Mean Absolute Error: {mae}\")\n",
        "print(f\"Root Mean Squared Error: {rmse}\")\n"
      ],
      "metadata": {
        "id": "bDXkzdbpsycB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from statsmodels.tools.tools import add_constant\n",
        "\n",
        "# Generate synthetic data\n",
        "X = pd.DataFrame({\n",
        "    'feature1': np.random.rand(100),\n",
        "    'feature2': np.random.rand(100),\n",
        "    'feature3': np.random.rand(100),\n",
        "    'feature4': np.random.rand(100),\n",
        "})\n",
        "\n",
        "# Add a constant to the model (intercept)\n",
        "X_const = add_constant(X)\n",
        "\n",
        "# Calculate VIF for each feature\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"Feature\"] = X_const.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X_const.values, i) for i in range(X_const.shape[1])]\n",
        "\n",
        "print(vif_data)\n"
      ],
      "metadata": {
        "id": "5C6ZLRczs0nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Generate synthetic data\n",
        "X = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
        "y = X**4 - 2*X**3 + X**2 + np.random.randn(100, 1) * 3  # Polynomial relationship with noise\n",
        "\n",
        "# Transform features to include polynomial terms (degree 4)\n",
        "poly = PolynomialFeatures(degree=4)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Fit polynomial regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_poly)\n",
        "\n",
        "# Plot the data points and the polynomial regression curve\n",
        "plt.scatter(X, y, color='blue')\n",
        "plt.plot(X, y_pred, color='red')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.title('Polynomial Regression (Degree 4)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "K0QhHWOys2Ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "tips = sns.load_dataset('tips')\n",
        "\n",
        "# Feature selection\n",
        "X = tips[['total_bill', 'size', 'sex', 'day']]\n",
        "y = tips['tip']\n",
        "\n",
        "# Convert categorical variable 'sex' and 'day' using one-hot encoding\n",
        "X = pd.get_dummies(X, drop_first=True)\n",
        "\n",
        "# Split data into training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('regressor', LinearRegression())\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "r2 = pipeline.score(X_test, y_test)\n",
        "print(f\"R-squared: {r2}\")\n"
      ],
      "metadata": {
        "id": "imqWrDQus4OD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Generate synthetic data\n",
        "X = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
        "y = X**3 - X**2 + X + np.random.randn(100, 1) * 3  # Polynomial relationship with noise\n",
        "\n",
        "# Transform features to include polynomial terms (degree 3)\n",
        "poly = PolynomialFeatures(degree=3)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Fit polynomial regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_poly)\n",
        "\n",
        "# Plot the data points and the polynomial regression curve\n",
        "plt.scatter(X, y, color='blue')\n",
        "plt.plot(X, y_pred, color='red')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.title('Polynomial Regression (Degree 3)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "NO8iz2kps5yi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate synthetic data with 5 features\n",
        "X = np.random.rand(100, 5)\n",
        "y = 2*X[:, 0] + 3*X[:, 1] - X[:, 2] + 0.5*X[:, 3] + 1.5*X[:, 4] + np.random.randn(100)\n",
        "\n",
        "# Split data into training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and fit the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Print the model's R-squared score and coefficients\n",
        "print(f\"R-squared: {model.score(X_test, y_test)}\")\n",
        "print(f\"Coefficients: {model.coef_}\")\n"
      ],
      "metadata": {
        "id": "b7x_Y0nps7HN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Generate synthetic data\n",
        "X = np.random.rand(100, 1) * 10\n",
        "y = 2 * X + 3 + np.random.randn(100, 1)\n",
        "\n",
        "# Fit the model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "# Plot the data points and the regression line\n",
        "plt.scatter(X, y, color='blue')\n",
        "plt.plot(X, y_pred, color='red')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.title('Linear Regression')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7U8ZJh9ss9Ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate synthetic data with 3 features\n",
        "X = np.random.rand(100, 3)\n",
        "y = 1.5*X[:, 0] + 2.5*X[:, 1] - 3.5*X[:, 2] + np.random.randn(100)\n",
        "\n",
        "# Split data into training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and fit the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Print the model's R-squared score and coefficients\n",
        "print(f\"R-squared: {model.score(X_test, y_test)}\")\n",
        "print(f\"Coefficients: {model.coef_}\")\n"
      ],
      "metadata": {
        "id": "ULtgcB_7s-i_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Create and fit the model\n",
        "model = LinearRegression()\n",
        "model.fit([[1], [2], [3]], [1, 2, 3])  # Example data\n",
        "\n",
        "# Save the model using pickle\n",
        "with open('linear_regression_model.pkl', 'wb') as f:\n",
        "    pickle.dump(model, f)\n",
        "\n",
        "# Load the model back from the file\n",
        "with open('linear_regression_model.pkl', 'rb') as f:\n",
        "    loaded_model = pickle.load(f)\n",
        "\n",
        "# Make predictions with the loaded model\n",
        "predictions = loaded_model.predict([[4]])\n",
        "print(f\"Predictions: {predictions}\")\n"
      ],
      "metadata": {
        "id": "jJNHGyARtAUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the 'tips' dataset\n",
        "tips = sns.load_dataset('tips')\n",
        "\n",
        "# Select features and target variable\n",
        "X = tips[['total_bill', 'size', 'sex', 'smoker', 'day', 'time']]\n",
        "y = tips['tip']\n",
        "\n",
        "# Define a pipeline to handle categorical variables with one-hot encoding\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('cat', OneHotEncoder(), ['sex', 'smoker', 'day', 'time']),\n",
        "            ('num', 'passthrough', ['total_bill', 'size'])\n",
        "        ])),\n",
        "    ('regressor', LinearRegression())\n",
        "])\n",
        "\n",
        "# Split the dataset into training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate the model\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Calculate the MSE\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse}\")\n"
      ],
      "metadata": {
        "id": "LsC3HR_vteT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import Ridge, LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate synthetic data\n",
        "X = np.random.rand(100, 3)\n",
        "y = 2 * X[:, 0] + 3 * X[:, 1] - X[:, 2] + np.random.randn(100)\n",
        "\n",
        "# Split data into training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit Linear Regression model\n",
        "lr_model = LinearRegression()\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "# Fit Ridge Regression model\n",
        "ridge_model = Ridge(alpha=1.0)\n",
        "ridge_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "lr_pred = lr_model.predict(X_test)\n",
        "ridge_pred = ridge_model.predict(X_test)\n",
        "\n",
        "# Evaluate the models\n",
        "lr_mse = mean_squared_error(y_test, lr_pred)\n",
        "ridge_mse = mean_squared_error(y_test, ridge_pred)\n",
        "\n",
        "print(f\"Linear Regression MSE: {lr_mse}\")\n",
        "print(f\"Ridge Regression MSE: {ridge_mse}\")\n",
        "print(f\"Linear Regression Coefficients: {lr_model.coef_}\")\n",
        "print(f\"Ridge Regression Coefficients: {ridge_model.coef_}\")\n"
      ],
      "metadata": {
        "id": "d9yNUQXOtgQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PNXgideUtg2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Generate synthetic data\n",
        "X = np.random.rand(100, 3)\n",
        "y = 2 * X[:, 0] + 3 * X[:, 1] - X[:, 2] + np.random.randn(100)\n",
        "\n",
        "# Create a linear regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Perform cross-validation\n",
        "scores = cross_val_score(model, X, y, cv=5, scoring='r2')\n",
        "\n",
        "print(f\"Cross-validated R-squared scores: {scores}\")\n",
        "print(f\"Mean R-squared: {scores.mean()}\")\n"
      ],
      "metadata": {
        "id": "9hmEuaNvth6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Generate synthetic data\n",
        "X = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
        "y = X**3 + np.random.randn(100, 1) * 10  # Polynomial relationship with noise\n",
        "\n",
        "# Split data into training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Loop through polynomial degrees and evaluate each\n",
        "for degree in range(1, 6):\n",
        "    poly = PolynomialFeatures(degree)\n",
        "    X_poly_train = poly.fit_transform(X_train)\n",
        "    X_poly_test = poly.transform(X_test)\n",
        "\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_poly_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_poly_test)\n",
        "\n",
        "    # Calculate R-squared score\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    print(f\"Degree {degree} Polynomial Regression R-squared: {r2}\")\n"
      ],
      "metadata": {
        "id": "g35_U_S_tjoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate synthetic data\n",
        "X = np.random.rand(100, 3)\n",
        "y = 2 * X[:, 0] + 3 * X[:, 1] - X[:, 2] + np.random.randn(100)\n",
        "\n",
        "# Add interaction terms\n",
        "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
        "X_interaction = poly.fit_transform(X)\n",
        "\n",
        "# Split data into training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_interaction, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and fit the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Print the coefficients\n",
        "print(\"Model Coefficients with Interaction Terms:\")\n",
        "print(model.coef_)\n"
      ],
      "metadata": {
        "id": "WVBE0aBYtkl-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}